{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "Este script hace la ejecución de Extracción, Procesamiento y Carga (ETL) de los archivos del sitio [Quién es Quién en los precios](https://datos.profeco.gob.mx/datos_abiertos/qqp.php).\n",
    "\n",
    "Una vez procesados estarán disponibles para la parte A y B del trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library installation of resources not in conda environment \"arquitectura\"\n",
    "%pip install awswrangler\n",
    "%pip install boto3\n",
    "%pip install rarfile\n",
    "%pip install selenium\n",
    "%pip install tqdm\n",
    "%pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import awswrangler as wr\n",
    "import boto3\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga y Preparacion de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargar los archivos desde el sitio [Quién es Quién en los precios](https://datos.profeco.gob.mx/datos_abiertos/qqp.php) y descomprimirlos en la carpeta `data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to ensure the data directory exists\n",
    "def ensure_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "# Base URL for making complete links\n",
    "base_url = \"https://datos.profeco.gob.mx/datos_abiertos/\"\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = \"https://datos.profeco.gob.mx/datos_abiertos/qqp.php\"\n",
    "\n",
    "# Ensure the data directory exists\n",
    "ensure_dir('data')\n",
    "\n",
    "# Send HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all <li> tags that contain <a> tags within them\n",
    "    links = soup.find_all('li')\n",
    "    file_links = []\n",
    "    \n",
    "    for link in tqdm(links):\n",
    "        a_tag = link.find('a', href=True)\n",
    "        if a_tag and 'file.php?t=' in a_tag['href']:\n",
    "            # Create the complete URL for the link\n",
    "            complete_url = base_url + a_tag['href']\n",
    "            file_links.append((complete_url, a_tag.text))\n",
    "\n",
    "    # Visit each link and download the file\n",
    "    for file_link, name in tqdm(file_links):\n",
    "        try:\n",
    "            # Make the request\n",
    "            response = requests.get(file_link)\n",
    "            # Save the content to a file\n",
    "            if response.status_code == 200:\n",
    "                file_path = os.path.join('data', name.replace('/', '_') + '.rar')  # Replace slashes just in case\n",
    "                with open(file_path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "                print(f\"File saved: {file_path}\")\n",
    "            else:\n",
    "                print(f\"Failed to download the file from {file_link}. Status code: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while downloading {file_link}: {str(e)}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desempacar los archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_rar_files(directory):\n",
    "    # Change this path to your 7-Zip executable if it's not in the PATH\n",
    "    seven_zip_path = \"7z\"\n",
    "    \n",
    "    # List all files in the given directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    # Filter for .rar files\n",
    "    rar_files = [file for file in files if file.endswith('.rar')]\n",
    "    \n",
    "    # Extract each .rar file\n",
    "    for rar in tqdm(rar_files):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory, rar)\n",
    "        # Command to extract the files\n",
    "        command = [seven_zip_path, 'x', file_path, '-o' + directory]\n",
    "        # Run the command\n",
    "        subprocess.run(command, check=True)\n",
    "\n",
    "# Replace 'data' with your directory path if different\n",
    "unpack_rar_files('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esquema de los archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the expected columns based on the provided schema\n",
    "expected_columns = [\n",
    "    'producto',\n",
    "    'presentacion',\n",
    "    'marca',\n",
    "    'categoria',\n",
    "    'catalogo',\n",
    "    'precio',\n",
    "    'fecha_registro',\n",
    "    'cadena_comercial',\n",
    "    'giro',\n",
    "    'nombre_comercial',\n",
    "    'direccion',\n",
    "    'estado',\n",
    "    'municipio',\n",
    "    'latitud',\n",
    "    'longitud'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directorio que contiene los archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the CSV files\n",
    "data_dir = 'data'  # Adjust this path as needed in your local setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontrar todos los archivos en el directorio \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find all CSV files in directory and subdirectories\n",
    "def find_csv_files(directory):\n",
    "    csv_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                csv_files.append(os.path.join(root, file))\n",
    "    return csv_files\n",
    "\n",
    "# Get all CSV files\n",
    "csv_files = find_csv_files(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and transform data\n",
    "def clean_data(data):\n",
    "    # Remove accents\n",
    "    for column in data.columns:\n",
    "        data[column] = data[column].apply(lambda x: unidecode(str(x)) if isinstance(x, str) and pd.notnull(x) else x)\n",
    "    # Convert to lowercase\n",
    "    data = data.apply(lambda x: x.str.lower() if x.dtype == \"object\" else x)\n",
    "    # Change commas to pipes in 'direccion', considering complex rules\n",
    "    # Updated regex pattern to avoid syntax error\n",
    "    data['direccion'] = data['direccion'].apply(\n",
    "        lambda x: re.sub(r',(?![^\"]*\"(?:(?:[^\"]*\"){2})*[^\"]*$)', '|', x) if pd.notnull(x) else x)\n",
    "    return data\n",
    "\n",
    "# Ensure the 'data_clean' folder exists\n",
    "clean_folder = 'data_clean'\n",
    "os.makedirs(clean_folder, exist_ok=True)\n",
    "\n",
    "# Read each file and process\n",
    "for file_path in tqdm(csv_files):\n",
    "    try:\n",
    "        # Read the file assuming no headers and using the expected columns\n",
    "        data = pd.read_csv(file_path, header=None, names=expected_columns)\n",
    "\n",
    "        # Clean and transform data\n",
    "        cleaned_data = clean_data(data)\n",
    "\n",
    "        # Ensure filename is included\n",
    "        cleaned_data['filename'] = os.path.basename(file_path)\n",
    "\n",
    "        # Ensure filename is the first column\n",
    "        cleaned_data = cleaned_data[['filename'] + [col for col in expected_columns if col in cleaned_data.columns]]\n",
    "\n",
    "        # Save the cleaned data to a new gzip file in the 'data_clean' directory\n",
    "        clean_file_path = os.path.join(clean_folder, os.path.basename(file_path).replace('.csv', '.csv.gz'))\n",
    "        cleaned_data.to_csv(clean_file_path, index=False, compression='gzip')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "\n",
    "print(\"Data processing completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar los archivos a S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a session\n",
    "session = boto3.Session(profile_name='arquitectura_AWS_ITAM_2024', region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an S3 client from this session\n",
    "s3 = session.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder to take and S3 bucket to upload\n",
    "folder_path = 'data_clean'\n",
    "bucket_name = 'mdge-e3-2024'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to upload files to S3|\n",
    "def upload_files_to_s3(folder_path, bucket_name):\n",
    "    \n",
    "    # Iterate over files in the directory\n",
    "    for filename in tqdm(os.listdir(folder_path)):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Check if it's a file\n",
    "        if os.path.isfile(file_path):\n",
    "            # Upload the file\n",
    "            try:\n",
    "                s3.upload_file(file_path, bucket_name, filename)\n",
    "                print(f\"Uploaded {filename} to S3 bucket {bucket_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to upload {filename}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload files\n",
    "upload_files_to_s3(folder_path, bucket_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
